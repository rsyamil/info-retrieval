{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "#from time import sleep\n",
    "import time as time\n",
    "import requests\n",
    "from random import randint\n",
    "from html.parser import HTMLParser\n",
    "import pickle\n",
    "\n",
    "USER_AGENT = {'User-Agent':'Emil'}\n",
    "\n",
    "#class for scraping\n",
    "class SearchEngine:\n",
    "    def search(query, sleep=True):\n",
    "        if sleep: #prevents loading too many pages too soon\n",
    "            time.sleep(randint(200, 400))\n",
    "        temp_url = '+'.join(query.split()) #for adding + between words for the query\n",
    "        url = 'https://www.bing.com/search?q=' + temp_url + '&count=30'\n",
    "        print(url)\n",
    "        soup = BeautifulSoup(requests.get(url, headers=USER_AGENT).text, 'html.parser')\n",
    "        new_results = SearchEngine.scrape_search_results(soup)\n",
    "        return new_results\n",
    "        \n",
    "    def scrape_search_results(soup):\n",
    "        raw_results = soup.find_all('li', {'class': 'b_algo'})\n",
    "        #print(raw_results)\n",
    "        results = []\n",
    "        for result in raw_results:\n",
    "            link = result.find('a').get('href')\n",
    "            results.append(link)\n",
    "        return results\n",
    "        \n",
    "#read the queries into an array\n",
    "def load_queries():\n",
    "    queries = []\n",
    "    #queries_lines = open('100QueriesSet1.txt', 'r')\n",
    "    queries_lines = open('100QueriesSet1_small.txt', 'r')\n",
    "    for q in queries_lines:\n",
    "        _ = q.strip()\n",
    "        queries.append(_)\n",
    "    return queries\n",
    "\n",
    "#read the json Google reference results\n",
    "def load_ref():\n",
    "    ref = open(\"Google_Result1.json\", \"r\")\n",
    "    ref_content = ref.read()\n",
    "    ref_dict = json.loads(ref_content)\n",
    "    return ref_dict\n",
    "\n",
    "#convert python dict to an array\n",
    "def dict2array(queries, ref_dict):\n",
    "    ref_array = []\n",
    "    for q in queries:\n",
    "        _ = ref_dict[q]\n",
    "        ref_array.append(_)\n",
    "    return ref_array\n",
    "\n",
    "#convert to lowercase > remove http or https > remove :// > remove www. > remove / at the end for search results or reference files \n",
    "def process_results(results_all):\n",
    "    processed_results = []\n",
    "    for rs in results_all:\n",
    "        rs_ = []\n",
    "        for r in rs:\n",
    "            r = r.lower()\n",
    "            if r.startswith('https'):\n",
    "                r = r[len('https'):]\n",
    "            if r.startswith('http'):\n",
    "                r = r[len('http'):]\n",
    "            r = r[len('://'):]\n",
    "            if r.startswith('www.'):\n",
    "                r = r[len('www.'):]\n",
    "            if r.endswith('/'):\n",
    "                r = r[:-1]\n",
    "            rs_.append(r)\n",
    "        processed_results.append(rs_)\n",
    "    return processed_results\n",
    "\n",
    "#remove duplicates, get first 10 results\n",
    "def remove_duplicates_truncate(result, result_original):\n",
    "    res = []\n",
    "    res_o = []\n",
    "    for r, r_o in zip(result, result_original):\n",
    "        if r not in res:\n",
    "            res.append(r)\n",
    "            res_o.append(r_o)\n",
    "    if len(res) > 10:\n",
    "        return res[0:10], res_o[0:10]\n",
    "    else:\n",
    "        return res, res_o\n",
    "\n",
    "#remove duplicates, get first 10 results\n",
    "def filter_results(results, results_original):\n",
    "    results_f = []\n",
    "    results_f_original = []\n",
    "    for result, result_original in zip(results, results_original):\n",
    "        _, _original = remove_duplicates_truncate(result, result_original)\n",
    "        results_f.append(_)\n",
    "        results_f_original.append(_original)\n",
    "    return results_f, results_f_original\n",
    "\n",
    "def dump_search_results(results_all_f, queries):\n",
    "    out = dict()\n",
    "    for idx, q in enumerate(queries):\n",
    "        out[q] = results_all_f[idx]\n",
    "    with open('hw1.json', 'w') as fp:\n",
    "        json.dump(out, fp, indent=4)\n",
    "        \n",
    "#calculate URL matches (n) \n",
    "def find_overlap(references, results):\n",
    "    \n",
    "    all_index_ref = []\n",
    "    all_index_res = []\n",
    "    \n",
    "    for ref, res in zip(references, results):\n",
    "        \n",
    "        index_ref = [] #index as it appears in the reference solution\n",
    "        index_res = [] #index as it appears in the scraped solution\n",
    "        \n",
    "        for index_in_ref, r in enumerate(ref):\n",
    "            if r in res:\n",
    "                index_in_res = res.index(r)\n",
    "                index_res.append(index_in_res)\n",
    "                index_ref.append(index_in_ref)\n",
    "                \n",
    "        all_index_ref.append(index_ref)\n",
    "        all_index_res.append(index_res)\n",
    "\n",
    "    return all_index_ref, all_index_res\n",
    "\n",
    "def _sum(arr): \n",
    "    sum=0\n",
    "    for i in arr:\n",
    "        sum = sum + i\n",
    "    return(sum) \n",
    "\n",
    "def sum_of_squares(x, y):\n",
    "    diff = [a_i - b_i for a_i, b_i in zip(x, y)]\n",
    "    diff_sq = [a_i**2 for a_i in diff]\n",
    "    return _sum(diff_sq)\n",
    "\n",
    "def rho(n, sos):\n",
    "    return 1 - ((6.0*sos)/(n*((n**2)-1)))\n",
    "\n",
    "#squared of rank differences (d^2)\n",
    "def calculate_metric(all_index_ref, all_index_res):\n",
    "    metrics = []\n",
    "    for ref, res in zip(all_index_ref, all_index_res):\n",
    "        #print(res)\n",
    "        #print(ref)\n",
    "        sos = sum_of_squares(ref, res)\n",
    "        n = len(ref)\n",
    "        percentage = (n/10.0)*100\n",
    "        if n == 1:\n",
    "            if sos == 0:\n",
    "                rho_ = 1.0\n",
    "            else:\n",
    "                rho_ = 0.0\n",
    "        elif n == 0:\n",
    "            rho_ = 1.0\n",
    "        else:\n",
    "            rho_ = rho(n, sos)\n",
    "        metrics.append((n, percentage, rho_))\n",
    "    return metrics\n",
    "\n",
    "def calculate_averages(metrics):\n",
    "    averages = []\n",
    "    for i in range(3):\n",
    "        sum_ = 0\n",
    "        for m in metrics:\n",
    "            sum_ = sum_ + m[i]\n",
    "        averages.append(1.0*sum_/len(metrics))\n",
    "    return averages\n",
    "\n",
    "def write_output(metrics, averages):\n",
    "    output = open('hw1.csv' , 'w')\n",
    "    output.write('Queries, Number of Overlapping Results, Percent Overlap, Spearman Coefficient\\n')\n",
    "    for idx, m in enumerate(metrics):\n",
    "        output.write('Query '+str(idx+1)+', '+str(m[0])+', '+str(m[1])+', '+str(m[2])+'\\n')\n",
    "    output.write('Averages, '+str(averages[0])+', '+str(averages[1])+', '+str(averages[2])+'\\n')\n",
    "    output.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bing.com/search?q=How+do+you+replace+coolant+thermostat&count=30\n",
      "No of results returned:  30\n",
      "https://www.bing.com/search?q=How+is+library+science+vand+information+science+related&count=30\n",
      "No of results returned:  29\n",
      "https://www.bing.com/search?q=Which+phase+is+the+non+dividing+stage&count=30\n",
      "No of results returned:  28\n",
      "https://www.bing.com/search?q=How+much+has+michael+vick+worth+after+release+from+prison&count=30\n",
      "No of results returned:  17\n"
     ]
    }
   ],
   "source": [
    "#load queries and reference answers\n",
    "queries = load_queries()\n",
    "ref_dict = load_ref()\n",
    "\n",
    "#scrape URLs from the webpage\n",
    "results_all = []\n",
    "for idx, q in enumerate(queries):\n",
    "    _ = SearchEngine.search(q)\n",
    "    print(\"Query: \" + idx + \" No of results returned: \" + len(_))\n",
    "    results_all.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.pkl','wb') as f:\n",
    "    pickle.dump(results_all, f)\n",
    "    \n",
    "with open('results.pkl','rb') as f:\n",
    "    results_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TASK 1 #######\n",
    "\n",
    "#process returned search results and reference answers\n",
    "results_all_p = process_results(results_all)\n",
    "ref_array = dict2array(queries, ref_dict)\n",
    "ref_array_p = process_results(ref_array)\n",
    "\n",
    "#get the first 10 and remove duplicates\n",
    "results_all_p_f, results_all_f = filter_results(results_all_p, results_all)\n",
    "\n",
    "#dump results into a json file\n",
    "dump_search_results(results_all_f, queries)\n",
    "\n",
    "#find overlap\n",
    "all_index_ref, all_index_res = find_overlap(ref_array_p, results_all_p_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### TASK 2 #######\n",
    "\n",
    "metrics = calculate_metric(all_index_ref, all_index_res)\n",
    "averages = calculate_averages(metrics)\n",
    "\n",
    "write_output(metrics, averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
